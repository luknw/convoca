import torch

from utils import *
from scipy.stats import entropy
from collections import Counter
from functools import wraps


def make_table_walk(nbins, known_rule=''):
    """
    Walk across a table of CA rules, changing one
    index at a time. When a specific rules is given, incorporate it into the walk

    nbins : the number of rules, and entries

    known_rule : np.array, a known rule to include (consisting of ones and zeros)


    Dev:
    - Work with list of rules rather than just one or zero specified rules
    - Check ordering of the rules; right now this only takes the outputs of the
    truth table and assumes the ordering generated by "all_combinations"
    - A better algorithm would traverse the rule list in one loop and draw
    indices to hit from different, non-overlapping sets based on the current index
    value. Probably not much performance boost though, but at least conceptually
    simpler
    """

    selection_order = np.random.choice(range(nbins), nbins, replace=False)

    all_rules = np.zeros((nbins, nbins))

    if len(known_rule) == 0:
        for ind in range(len(all_rules)):
            all_rules[ind:, selection_order[ind]] = 1
    else:

        num_on = int(np.sum(known_rule))
        num_off = int(nbins - num_on)

        where_on = np.where(known_rule == 1)[0]
        where_off = np.where(~(known_rule == 1))[0]

        assert num_on == len(where_on)
        assert num_off == len(where_off)

        selection_order_indices = np.random.choice(range(num_on), num_on, replace=False)
        selection_order = where_on[selection_order_indices]
        for ind in range(len(selection_order)):
            all_rules[ind:, selection_order[ind]] = 1

        selection_order_indices = np.random.choice(range(num_off), num_off, replace=False)
        selection_order = where_off[selection_order_indices]
        for ind in range(len(selection_order)):
            all_rules[num_on + ind:, selection_order[ind]] = 1

    return all_rules


def get_network_entropies(layers_samples_neurons):
    neuron_entropies_by_layer = []
    layer_entropies = []
    
    for l in layers_samples_neurons:
        p_neuron = l.mean(axis=0)
        neuron_entropies_by_layer.append(entropy([p_neuron, 1-p_neuron], base=2))
        
        layer_patterns = (tuple(sample) for sample in l)
        layer_pattern_counts = list(Counter(layer_patterns).values())
        layer_entropies.append(entropy(layer_pattern_counts, base=2))
        
    network_patterns = (tuple(sample.ravel()) for sample in layers_samples_neurons.swapaxes(0, 1))
    network_pattern_counts = list(Counter(network_patterns).values())
    network_entropy = entropy(network_pattern_counts, base=2)
    
    return network_entropy, layer_entropies, neuron_entropies_by_layer
    

def periodic_padding(image, padding=1):
    """
    Create a periodic padding (wrap) around an image stack, to emulate periodic boundary conditions
    Adapted from https://github.com/tensorflow/tensorflow/issues/956

    If the image is 3-dimensional (like an image batch), padding occurs along the last two axes


    """
    if len(image.shape) == 2:
        upper_pad = image[-padding:, :]
        lower_pad = image[:padding, :]

        partial_image = torch.cat([upper_pad, image, lower_pad], dim=0)

        left_pad = partial_image[:, -padding:]
        right_pad = partial_image[:, :padding]

        padded_image = torch.cat([left_pad, partial_image, right_pad], dim=1)

    elif len(image.shape) == 3:
        upper_pad = image[:, -padding:, :]
        lower_pad = image[:, :padding, :]

        partial_image = torch.cat([upper_pad, image, lower_pad], dim=1)

        left_pad = partial_image[:, :, -padding:]
        right_pad = partial_image[:, :, :padding]

        padded_image = torch.cat([left_pad, partial_image, right_pad], dim=2)

    else:
        raise Exception("Input data shape not understood.")

    return padded_image


def conv_cast(arr):
    return torch.from_numpy(arr).float()


def arr2tf(arr):
    """Given np.array, convert to a float32 tensor

    var_type: 'var' or 'const'
        Whether the created variable is a constant of fixed

    """

    arr_tf = torch.from_numpy(arr)
    return arr_tf.type.float()


def categorize_images(image_stack, neighborhood="von neumann"):
    """
    Given an MxNxN stack of numpy images, performs periodic convolution with an SxSxT
    stack of kernels to produce an MxNxN output representing which of the T classes
    each pixel belongs to. Each class represents a distinct neighborhood arrangement
    around that point

    This function may be used to find the prior distribution of inputs in an image

    Returns
    -------

    indices : tf.Tensor. Corresponds to the T labels for each pixel in the original
    image stack

    """

    if neighborhood == "von neumann":
        pad_size = 1
        all_filters = all_combinations(2, (3, 3))
        all_biases = 1 - np.sum(all_filters, axis=(1, 2))
        all_filters[all_filters == 0] -= np.prod(all_filters.shape[1:])
    else:
        raise Exception("Specified neighborhood type not implemented")

    state = conv_cast(image_stack)
    kernel = torch.unsqueeze(conv_cast(all_filters), dim=2)
    biases = conv_cast(all_biases)

    input_padded = torch.unsqueeze(periodic_padding(state, pad_size), dim=1)

    conv_image = torch.nn.functional.conv2d(input_padded, kernel)

    # last axis is one-hot representation telling us which of the D^M states we are in
    activation_image = torch.nn.functional.relu(conv_image + biases)

    indices = torch.argmax(activation_image, dim=-1)

    return indices


def image_entropy(im_stack, neighborhood="von neumann"):
    """
    Given a stack of images, compute the entropy of the symbol distribution for
    each image. Currently, this function assumes a von Neumann neighborhood
    around each pixel

    im_stack : MxNxN np.array, where M indexes the image batch
        and NxN are the image dimensions

    Development
    -----------

    It would be nice if this whole process was pure Tensorflow, for speed

    """

    categ_im = categorize_images(im_stack)
    categ_im_arr = categ_im.numpy()

    flat_categs = np.reshape(categ_im_arr, (categ_im_arr.shape[0], np.prod(categ_im_arr.shape[-2:])))

    all_ents = np.zeros(flat_categs.shape[0])

    for ind, flat_thing in enumerate(flat_categs):
        unique_keys, counts = np.unique(flat_thing, return_counts=True)
        counts = counts.astype(float)
        # dict(zip(unique_keys, counts)) # make histogram dict

        counts /= np.sum(counts)  # normalize
        ent = entropy(counts, base=2)

        all_ents[ind] = ent

    return all_ents


def make_ca(words, symbols, neighborhood="von neumann"):
    """
    Build an arbitrary cellular automaton in tensorflow
    The CA will take images of the form MxNxN as input,
    where M is the batch size and NxN is the image dimensions

    CA states are formulated as individual "rules" based
    on pattern matching 2^D = 2^9 single inputs

    Inputs
    ------

    words: iterable of M x (...) input states corresponding to the
    rule table for the CA

    symbols : M-vector of assignments (next states) for each of the
    words, in the same order as the words vector

    Returns
    -------

    my_ca : func. A function in Tensorflow

    Development
    -----------

    Test to ensure that the generated function performs in both
    eager and traditional tensorflow environments

    """

    # this may not be true for a non-binary CA; generalize this later
    all_filters = words
    state_assignments = symbols

    if neighborhood == "von neumann":
        pad_size = 1
        all_filters = all_combinations(2, (3, 3))
        all_biases = 1 - np.sum(all_filters, axis=(1, 2))
        all_filters[all_filters == 0] -= np.prod(all_filters.shape[1:])
    else:
        assert True, "Specified neighborhood type not implemented"

    kernel = torch.unsqueeze(conv_cast(all_filters), dim=1)
    biases = conv_cast(all_biases)
    state_assignments = conv_cast(state_assignments)

    def my_ca(image_stack):
        """
        Automatically generated function created by make_ca()
        Input array must already be a tensor when fed to the function
        """
        input_padded = torch.unsqueeze(periodic_padding(image_stack, pad_size), dim=1)

        i = input_padded.float()
        k = kernel.float()
        b = biases.float()

        conv_image = torch.nn.functional.conv2d(i, k, b)

        # last axis is one-hot representation telling us which of the D^M states we are in
        activation_image = torch.nn.functional.relu(conv_image)

        state_assigment = torch.unsqueeze(torch.unsqueeze(state_assignments, dim=1), dim=2)
        next_states = torch.sum(activation_image * state_assigment, dim=1)

        return next_states

    return my_ca


def make_game_of_life():
    """
    Returns a simplified Tensorflow implementation of Conway's Game of Life
    """

    neighborhood_radius = 3
    pad_size = 1

    neighbor_filt = np.ones((neighborhood_radius, neighborhood_radius))
    neighbor_filt[1, 1] = 0
    middle_filt = np.zeros((neighborhood_radius, neighborhood_radius))
    middle_filt[1, 1] = 1
    all_filters = np.dstack((middle_filt, neighbor_filt, neighbor_filt, neighbor_filt, neighbor_filt))
    all_biases = np.array([0, -1, -2, -3, -4])
    total_filters = len(all_biases)
    kernel = torch.unsqueeze(conv_cast(all_filters), dim=2)
    biases = conv_cast(all_biases)

    wh1_arr = np.array([
        [0, 0, 4 / 3, -8 / 3, -1 / 3],
        [3 / 2, 5 / 4, -5, -1 / 4, -1 / 4]
    ]).T
    bh1_arr = np.array([-1 / 3, -7 / 4]).T
    wh1 = conv_cast(wh1_arr)
    bh1 = conv_cast(bh1_arr)

    def my_ca(image_stack):
        """
        Automatically generated function created by make_ca()
        Input array must already be a tensor when fed to the function
        """
        input_padded = torch.unsqueeze(periodic_padding(image_stack, pad_size), dim=1)

        conv_image = torch.nn.functional.conv2d(input_padded, kernel)

        activation_image = torch.nn.functional.relu(conv_image + biases)

        activated_flat = torch.reshape(activation_image, [-1, total_filters])

        h1 = torch.nn.functional.relu(torch.matmul(activated_flat, wh1) + bh1)

        scores = torch.sum(h1)
        next_states = torch.reshape(scores, [*activation_image.shape[:3], 1])

        return torch.squeeze(next_states)

    return my_ca


def make_glider(dims0):
    """
    Produce Glider initial conditions for Conway's Game of Life

    dims0 : int, float, or length 2 iterable

    """

    dims = np.ravel(np.array([dims0]))

    if len(dims) == 1:
        dims = np.squeeze([dims, dims])
    dims = np.array(dims)

    # Check that provided dimensions are large enough
    for item in dims:
        assert item >= 3

    glider_center = np.array([[0, 1, 0],
                              [0, 0, 1],
                              [1, 1, 1]])

    ins_inds = np.floor(dims / 2).astype(int)

    out_arr = np.zeros(dims)
    out_arr[ins_inds[0] - 1:ins_inds[0] + 2, ins_inds[1] - 1:ins_inds[1] + 2] = glider_center

    return out_arr

def make_batching_ca(state_transition, state_dims=2):
    """
    Applies ca over batches of states,
    so that it's compatible with the rest of the APIs.
    """
    @wraps(state_transition)
    def batching_ca(states):
        new_states = []
        for state in states.reshape(-1, *states.shape[-state_dims:]):
            new_states.append(state_transition(state))
        return np.array(new_states).reshape(states.shape)
    return batching_ca

def n_sum_ca(born, survives):
    @make_batching_ca
    def ca(state):
        kernel = np.zeros_like(state)
        m, n = kernel.shape
        kernel[m//2-1 : m//2+2, n//2-1 : n//2+2] = np.pad([[0]], 1, constant_values=1)

        neighbours_alive = fft_convolve2d(state, kernel)

        new_state = np.zeros_like(neighbours_alive)
        new_state[np.where((state == 0) & np.isin(neighbours_alive, born))] = 1
        new_state[np.where((state == 1) & np.isin(neighbours_alive, survives))] = 1

        return new_state
    return ca

def plot_state(state):
    clear_output(wait=True)
    plt.imshow(state, vmin=0, vmax=1)
    plt.axis(False)
    plt.show()

def run_ca(ca, size=(100, 100), p_alive=0.5, iters=None):
    iters = range(iters) if iters else count()
    
    state = np.random.choice([0, 1], size=size, p=[1-p_alive, p_alive])
    m, n = state.shape
    state[m//2, n//2] = 1 if p_alive > 0 else 0 # ensure at least 1 living cell
    plot_state(state)
    for _ in iters:
        time.sleep(0.1)
        state = ca(state)
        plot_state(state) 