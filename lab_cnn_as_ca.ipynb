{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network as Cellular Automata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: https://doi.org/10.1103/PhysRevE.100.032402"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quoting Wikipedia:\n",
    "\n",
    "> A cellular automaton consists of a regular grid of cells, each in one of a finite number of states, such as on and off [...]. The grid can be in any finite number of dimensions. For each cell, a set of cells called its neighbourhood is defined relative to the specified cell. An initial state (time t = 0) is selected by assigning a state for each cell. A new generation is created (advancing t by 1), according to some fixed rule (generally, a mathematical function) that determines the new state of each cell in terms of the current state of the cell and the states of the cells in its neighbourhood. Typically, the rule for updating the state of cells is the same for each cell and does not change over time, and is applied to the whole grid simultaneously [...]\n",
    ">\n",
    "> <cite>https://en.wikipedia.org/wiki/Cellular_automaton</cite>\n",
    "\n",
    "The important thing here is that cellular automatons are local, i.e. the next state of each cell depends only on the cells in its neighbourhood. This principle of locality is also a basis for applying convolutional filters, which combine information from a single part of the image at a time. Expanding on this similarity, we can ponder whether cellular automatons and convolutional neural networks have more things in common. Let's check this out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAs and convolutions in plain Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a few simple CAs to learn the techniques we'll be using further. The CAs will:\n",
    "* be defined on a square grid\n",
    "* have 2 states - so called binary CA, the states are usually called dead (0) and alive (1)\n",
    "* use Moore neighbourhood - a square of 3 by 3 cells with a cell of reference in the centre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/luknw/convoca.git\n",
    "%cd convoca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['figure.figsize'] = (7, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Rule-based CA\n",
    "The simplest automaton can be defined by pattern-matching rules for each possible input. No big deal, we just need to pay attention to boundary conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_pytorch.ca_funcs import run_ca, make_ca\n",
    "\n",
    "def k(a):\n",
    "    \"\"\"\n",
    "    Convert array to a tuple for use as a hashable key.\n",
    "    \"\"\"\n",
    "    return tuple(np.array(a).ravel())\n",
    "\n",
    "@make_ca\n",
    "def triangles(state):\n",
    "    born = set(k(s) for s in (\n",
    "        [[0, 0, 1], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, 1]],\n",
    "        [[0, 0, 0], [1, 0, 0], [0, 0, 0]],\n",
    "    ))\n",
    "    \n",
    "    new_state = np.zeros_like(state)\n",
    "\n",
    "    # TODO: We want to have a state grid that's _wrapped_ around,\n",
    "    # left to right and top to bottom, so that CA rules\n",
    "    # can be safely applied at the boundary.\n",
    "    # Hint: use np.pad with the right mode.\n",
    "    state_pad = np.pad(state, 1, 'wrap')\n",
    "\n",
    "    for i in range(state.shape[0]):\n",
    "        for j in range(state.shape[1]):\n",
    "            if k(state_pad[i:i+3, j:j+3]) in born:\n",
    "                new_state[i][j] = 1\n",
    "    \n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ca(triangles, p_alive=0.0003, iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Convolution\n",
    "Let's build a simple convolution which sums the numbers in a 3 by 3 square just to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "\n",
    "def conv_sum_neighbours(a):\n",
    "    # TODO: Define a convolution kernel that will sum all values\n",
    "    # inside a 3x3 block apart from the center square.\n",
    "    # Hint: the kernel will be a 3x3 array with 1s and 0s in the right places.\n",
    "    kernel = np.pad([[0]], 1, constant_values=1)\n",
    "    return convolve2d(a, kernel, mode='valid')\n",
    "\n",
    "# test inputs have size 3x3 just to make things simple\n",
    "def conv_test():\n",
    "    x = np.arange(1, 10).reshape(3, 3)\n",
    "    print(x)\n",
    "    print(x.sum(), conv_sum_neighbours(x))\n",
    "    \n",
    "    eq = []\n",
    "    for i in range(10):\n",
    "        x = np.random.rand(3, 3)\n",
    "        eq.append(np.isclose(conv_sum_neighbours(x), x.sum() - x[1, 1]))\n",
    "    print(all(eq))\n",
    "\n",
    "conv_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Totalistic CA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Totalistic CAs don't care about layout of cells in a neighbourhood, they just compute some neighbourhood statistic like a number of living cells. As we already know, the sum of values in the neighbouring cells is a perfect use case for a convolution!\n",
    "\n",
    "The convolution will be computed for all cells at once using arcane knowledge of the fast Fourier transform. This also has a nice side effect that we get wrapped boundaries for free. It's not that important how this FFT works, you can treat it as a black box. The important bit is that we use a convolution to implement a CA transition function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Conway's game of Life\n",
    "The most famous cellular automaton that exhibits complex behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_pytorch.utils import fft_convolve2d\n",
    "\n",
    "@make_ca\n",
    "def conway(state):\n",
    "    kernel = np.zeros_like(state)\n",
    "    m, n = kernel.shape\n",
    "    \n",
    "    # TODO: define a 3x3 convolution kernel that will count living neighbours (state := 1) of the cell\n",
    "    kernel[m//2-1 : m//2+2, n//2-1 : n//2+2] = np.pad([[0]], 1, constant_values=1)\n",
    "\n",
    "    # 2d table with counts of living neighbours for each corresponding cell from the state\n",
    "    neighbours_alive = fft_convolve2d(state, kernel)\n",
    "    \n",
    "    new_state = np.zeros_like(neighbours_alive)\n",
    "    \n",
    "    born = [3]\n",
    "    survives = [2, 3]\n",
    "    # TODO: place born & survives arrays in the right places\n",
    "    # states: 0 - dead, 1 - alive\n",
    "    new_state[(state == 0) & np.isin(neighbours_alive, born)] = 1\n",
    "    new_state[(state == 1) & np.isin(neighbours_alive, survives)] = 1\n",
    "\n",
    "    return new_state   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ca(conway, iters=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day and night automaton\n",
    "We've seen the `triangles` CA which behaves orderly up to a point, but then quite reliably transforms into a random mess and also the `conway` CA that can have runs of varying length and complexity. What determines the CA complexity? Let's compare a few other automatons with rules that differ only slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: CA comparison\n",
    "Run each of the following CAs and just observe the results. The first CA is called \"day and night\", becuase it exhibits symmetry between dead and alive states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_pytorch.ca_funcs import n_sum_ca\n",
    "\n",
    "day_and_night = [\n",
    "    (f'day_and_night_{i}', n_sum_ca(born, survives))\n",
    "    for i, (born, survives)\n",
    "    in enumerate([\n",
    "        ([3, 6, 7, 8], [3, 4, 6, 7, 8]),\n",
    "        ([3, 4, 6, 7, 8], [3, 4, 6, 7, 8]),\n",
    "        ([3, 6, 7, 8], [3, 6, 7, 8]),\n",
    "        ([3, 4, 6, 7, 8], [3, 6, 7, 8])\n",
    "    ])\n",
    "]\n",
    "\n",
    "def run(i, **kwargs):\n",
    "    name, ca = day_and_night[i]\n",
    "    run_ca(ca, **kwargs)\n",
    "    print(name, kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(0, iters=100, p_alive=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(1, iters=100, p_alive=0.125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(2, iters=100, p_alive=0.875)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(3, iters=30, p_alive=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(3, iters=100, p_alive=0.78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(3, iters=100, p_alive=0.22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, modifying the rules slightly can change the automaton behaviour a lot. In case of our automata, we could probably start building intuition about the outcomes, but the symmetry we encountered is just the matter of this specific \"day and night\" automaton. To make matters worse, the last example shows that the behaviour can vary based on the input distribution... and we're only looking at totalistic automata. Things get even more unpredictable for rule-based CAs.\n",
    "\n",
    "How to quantify this? How to quantify the complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wolfram classification & entropy\n",
    "Approaching the problem differently, we could try to describe automatons qualitatively, by pointing out the kinds of outcomes they produce. The most prominent classification was proposed by Wolfram:\n",
    "\n",
    "### Wolfram classification\n",
    "* Class 1: Nearly all initial patterns evolve quickly into a stable, homogeneous state. Any randomness in the initial pattern disappears.\n",
    "* Class 2: Nearly all initial patterns evolve quickly into stable or oscillating structures. Some of the randomness in the initial pattern may filter out, but some remains. Local changes to the initial pattern tend to remain local.\n",
    "* Class 3: Nearly all initial patterns evolve in a pseudo-random or chaotic manner. Any stable structures that appear are quickly destroyed by the surrounding noise. Local changes to the initial pattern tend to spread indefinitely.\n",
    "* Class 4: Nearly all initial patterns evolve into structures that interact in complex and interesting ways, with the formation of local structures that are able to survive for long periods of time.\n",
    "\n",
    "The descriptions are from: https://en.wikipedia.org/wiki/Cellular_automaton#Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "To quantify the automaton complexity, we can look at all possible automaton input neighbourhoods, apply the CA transformation to each one of them, each time producing as output a new state for a single cell. $\\lambda$ is defined as the ratio of such output cells that are alive. $\\lambda = 0$ when all outputs are dead, $\\lambda = 1$ when all are alive, and $\\lambda = 0.5$ when the two outcomes are balanced. A rule of thumb is that the closer $\\lambda$ is to 0.5, the greater the automaton complexity and the higher Wolfram's class.\n",
    "\n",
    "We won't use $\\lambda$ directly, however we'll use a metric that's correlated with it: the automaton entropy. Let's assume we apply a CA transition once to an input that has a uniform distribution of all possible neighbourhoods. After the one CA transition, for each possible neighbourhood $\\sigma$ we calculate its probability of appearing in the output $p_\\sigma$ and calculate the entropy of their distribution. The number we get is called automaton entropy:\n",
    "\n",
    "$$H_{ca} = -\\sum_{\\sigma} p_\\sigma \\log_2{p_\\sigma}$$\n",
    "\n",
    "For binary automata (2 states) with Moore neighborhood (9 cells), the entropy ranges from 0 (all inputs map to the same output neighbourhood) to 9 (uniform neighbourhood distribution in output). Let's find the value of entropy for the automata we already know. Since synthesising an image with a uniform input distribution is complicated, we'll approximate it by applying the CA to all possible input neighbourhoods wrapped around one by one and then adding up the counts of output neighbourhoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Calculating automaton entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from scipy.stats import entropy\n",
    "from collections import Counter\n",
    "\n",
    "def all_combinations(m, shape):\n",
    "    '''\n",
    "    Make an array of all `shape` dimensional inputs\n",
    "    consisting of m possible values\n",
    "    '''    \n",
    "    indices = np.tile(np.array([np.arange(m)]).T, np.product(shape))\n",
    "    all_combos = list(product(*list(indices.T)))\n",
    "    return np.reshape(np.array(all_combos), (-1, *shape))\n",
    "\n",
    "def simple_ca_entropy(ca):\n",
    "    # TODO: Provide the right parameters for the functions.\n",
    "    outputs = Counter(k(ca(c)) for c in all_combinations(2, (3, 3)))\n",
    "    return entropy(list(outputs.values()), base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('triangles', simple_ca_entropy(triangles))\n",
    "print('conway', simple_ca_entropy(conway))\n",
    "for name, ca in day_and_night:\n",
    "    print(name, simple_ca_entropy(ca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN as CA\n",
    "\n",
    "Convolutional Neural Networks demonstrated great success in the modelling of complex systems, like speech patterns\n",
    "and physics simulations. We can try to use them in the prediction of the output of a CA after one step.\n",
    "\n",
    "### Network model\n",
    "\n",
    "We will use one of the network architectures proposed by the authors of the paper:\n",
    "![CNN architecture](https://journals.aps.org/pre/article/10.1103/PhysRevE.100.032402/figures/2/medium \"CNN architecture\")\n",
    "\n",
    "We will use PyTorch in these examples as it allows us to trace operations on the lower level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from lab_pytorch.train_ca import initialize_model\n",
    "from lab_pytorch.ca_funcs import make_table_walk, make_ca\n",
    "\n",
    "seed = 0\n",
    "print('seed =', seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define various hyper-parameters of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "samples = 500\n",
    "input_dims = [10, 10]\n",
    "layer_dims = [100] + [100] * 11\n",
    "batch_size = 10\n",
    "num_batches = samples // batch_size\n",
    "learning_rate = 1e-4\n",
    "training_epochs = 100\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = initialize_model(input_dims, layer_dims)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Basic training\n",
    "\n",
    "We start by defining utility methods for data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_CA_train_data(ca, height=10, width=10, n_samples=500):\n",
    "    X_train = torch.from_numpy(np.random.choice([0, 1], (n_samples, height, width), p=[.5, .5])).float()\n",
    "    Y_train = ca(X_train)\n",
    "    if not isinstance(Y_train, torch.Tensor):\n",
    "        Y_train = torch.from_numpy(Y_train)\n",
    "    Y_train = Y_train.float()\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can define the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def learn_CA(ca, model, optimizer, training_epochs):\n",
    "    losses = []\n",
    "    X_train, Y_train = generate_CA_train_data(ca, *input_dims, n_samples=samples)\n",
    "    if torch.cuda.is_available():\n",
    "        X_train = X_train.cuda()\n",
    "        Y_train = Y_train.cuda()\n",
    "\n",
    "    for _ in tqdm(range(training_epochs)):\n",
    "        batch_losses = []\n",
    "        for i in range(num_batches):\n",
    "            X_batch = X_train[i * batch_size : (i + 1) * batch_size]\n",
    "            Y_batch = Y_train[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            Y_pred = model(X_batch)\n",
    "            l = loss(Y_batch, Y_pred)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            batch_losses.append(l.item())\n",
    "        losses.append(np.mean(batch_losses))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "We can train the network and plot the loss function values. We will be training on a randomly created CA with a high entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ca = day_and_night[0][1]\n",
    "losses = learn_CA(ca, model, optimizer, training_epochs)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.loglog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we can check the output of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from lab_pytorch.ca_funcs import make_glider\n",
    "\n",
    "def display_sample_output(ca, model, use_glider=True):\n",
    "    if use_glider:\n",
    "        x = make_glider(10)\n",
    "        X_test = torch.from_numpy(x.reshape(1, 10, 10)).float()\n",
    "        Y_test = ca(X_test)\n",
    "    else:\n",
    "        X_test, Y_test = generate_CA_train_data(ca, *input_dims, n_samples=1)\n",
    "\n",
    "    if not isinstance(Y_test, torch.Tensor):\n",
    "        Y_test = torch.from_numpy(Y_test)\n",
    "    Y_test = Y_test.float()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        X_test = X_test.cuda()\n",
    "    Y_pred = model(X_test)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        X_test = X_test.cpu()\n",
    "        Y_pred = Y_pred.cpu()\n",
    "\n",
    "    X_test = X_test.detach().numpy()\n",
    "    Y_test = Y_test.detach().numpy()\n",
    "    Y_pred = Y_pred.detach().numpy()\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(X_test[0], vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Input\")\n",
    "\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(Y_test[0], vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Expected Output\")\n",
    "\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(Y_pred[0], vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Observed Output\")\n",
    "\n",
    "    plt.subplot(144)\n",
    "    plt.imshow((Y_pred[0] - Y_test[0]) ** 2)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Normalised Loss\")\n",
    "    \n",
    "    print('max loss:', ((Y_pred[0] - Y_test[0]) ** 2).max())\n",
    "\n",
    "display_sample_output(ca, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Calculating entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, we can calculate the network entropies and compare them with the automaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from lab_pytorch.ca_funcs import get_network_entropies\n",
    "\n",
    "def calculate_entropies(model):\n",
    "    def get_activations(x_input):\n",
    "        activations = []\n",
    "        for m in model.children():\n",
    "            x_input = m(x_input)\n",
    "            activations.append(x_input)\n",
    "        return activations[1:-3:2]\n",
    "\n",
    "    X_test = np.pad(all_combinations(2, (3, 3)), [(0, 0), (3, 4), (3, 4)], 'wrap')\n",
    "    X_test = torch.from_numpy(X_test).float()\n",
    "    if torch.cuda.is_available():\n",
    "        X_test = X_test.cuda()\n",
    "\n",
    "    res = [activation.cpu().detach().numpy() for activation in get_activations(X_test)]\n",
    "    layer_activations = np.array(res)\n",
    "    # TODO: Layer activations are floats, but to calculate entropy,\n",
    "    # we want to map activations to binary values,\n",
    "    # 1 if a given activation is >0, and 0 otherwise.\n",
    "    # Provide the correct parameters for np.digitize.\n",
    "    binary_activations = np.digitize(layer_activations, [0], right=True)\n",
    "    binary_activations = binary_activations.transpose(0, 1, -2, -1, 2) \\\n",
    "        .reshape(len(layer_dims), np.product(X_test.shape), layer_dims[0])\n",
    "    return get_network_entropies(binary_activations)\n",
    "\n",
    "calculate_entropies(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ca_entropy(ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Compression\n",
    "We can compare the results of training for high and low entropy CAs. The network used is very large, so it should be possible ot reduce its size after training. We will use network compression methods available in the nni library. Sparsity is a parameter that tells what percentage of neurons we aim to cut from the network. Find a high value that still preserves correctness. Are the sparsity values different for CAs with a high/low entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from nni.compression.torch import *\n",
    "\n",
    "def prune_model_and_test(model, Pruner, config):\n",
    "    model_copy = copy.deepcopy(model)\n",
    "    pruner = Pruner(model_copy, config, optimizer=optimizer)\n",
    "    pruner.compress()\n",
    "    return model_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High entropy CA (day_and_night)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config_list = [{ 'sparsity': 0.1, 'op_types': ['default'] }]\n",
    "m = prune_model_and_test(model, LevelPruner, config_list)\n",
    "display_sample_output(ca, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_sample_output(ca, m, use_glider=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low entropy CA (triangles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model2 = initialize_model(input_dims, layer_dims)\n",
    "if torch.cuda.is_available():\n",
    "    model2.cuda()\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = learn_CA(triangles, model2, optimizer2, training_epochs)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.loglog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display_sample_output(triangles, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display_sample_output(triangles, model2, use_glider=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "calculate_entropies(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config_list = [{ 'sparsity': 0.1, 'op_types': ['default'] }]\n",
    "m2 = prune_model_and_test(model2, LevelPruner, config_list)\n",
    "display_sample_output(triangles, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display_sample_output(triangles, model2, use_glider=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
